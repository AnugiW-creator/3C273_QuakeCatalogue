# -*- coding: utf-8 -*-
"""NSAC trial code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KFcGjdFdS9JLQhcKMddqbiXsNc1I1g-0

## Readme
The table of contents can be found on the left of the google colab file.
To use the file, you need to change the directory to your own directory/ upload the datasets to your google drive.
<br><br>
To use the machine learning models, you will need to run the cells in:

*   Import the libraries
*   Mount Google Drive to run code on Google Colab (if you plan to run the file on Google Colab)
*   Reading the data
*   Machinie Learning
*   Using the model to predict the results of test datasets
<br><br>

There are two models in this file, Support Vector Regression (SVR) and Long Short Term Memory (LSTM). Some cells are marked as to be used only with either SVR or LSTM models.

*   As the number of training datasets is very small, SVR will be more effective.
*   LSTM is effective at capturing sequential dependencies in data, but needs larger datasets.

<br><br>
Due to time constraint, we could not do analysis on the extra seismic events data.

### Import the libraries
"""

# Import libraries
!pip install obspy
import numpy as np
import pandas as pd
from obspy import read
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import os
import tensorflow as tf
from tensorflow import keras

"""### Mount Google Drive to run code on google colab"""

from google.colab import drive

drive.mount('/content/drive')

"""### Reading the data"""

path = '/content/drive/MyDrive/space_apps_2024_seismic_detection/data/lunar/training/catalogs'

cat = pd.read_csv(path + '/apollo12_catalog_GradeA_final.csv')
cat.head()

"""###Select Detection"""

row = cat.iloc[6]
arrival_time = datetime.strptime(row['time_abs(%Y-%m-%dT%H:%M:%S.%f)'],'%Y-%m-%dT%H:%M:%S.%f')
arrival_time

# If we want the value of relative time, we don't need to use datetime
arrival_time_rel = row['time_rel(sec)']
arrival_time_rel

# Let's also get the name of the file
test_filename = row.filename
test_filename

"""### Read CSV"""

data_directory = '/content/drive/MyDrive/space_apps_2024_seismic_detection/data/lunar/training/data/S12_GradeA/'
csv_file = f'{data_directory}{test_filename}.csv'
data_cat = pd.read_csv(csv_file)
data_cat

"""#### relative time"""

# Read in time steps and velocities
csv_times = np.array(data_cat['time_rel(sec)'].tolist())
csv_data = np.array(data_cat['velocity(m/s)'].tolist())

# Plot the trace!
fig,ax = plt.subplots(1,1,figsize=(10,3))
ax.plot(csv_times,csv_data)

# Make the plot pretty
ax.set_xlim([min(csv_times),max(csv_times)])
ax.set_ylabel('Velocity (m/s)')
ax.set_xlabel('Time (s)')
ax.set_title(f'{test_filename}', fontweight='bold')

# Plot where the arrival time is
arrival_line = ax.axvline(x=arrival_time_rel, c='red', label='Rel. Arrival')
ax.legend(handles=[arrival_line])

"""#### absolute time"""

# Read in time steps and velocities
csv_times_dt = []
for absval_str in data_cat['time_abs(%Y-%m-%dT%H:%M:%S.%f)'].values:
    csv_times_dt.append(datetime.strptime(absval_str,'%Y-%m-%dT%H:%M:%S.%f'))

csv_data = np.array(data_cat['velocity(m/s)'].tolist())

# Plot the trace!
fig,ax = plt.subplots(1,1,figsize=(10,3))
ax.plot(csv_times_dt,csv_data)

# Make the plot pretty
ax.set_xlim((np.min(csv_times_dt),np.max(csv_times_dt)))
ax.set_ylabel('Velocity (m/s)')
ax.set_xlabel('Time (month-day hour)')
ax.set_title(f'{test_filename}', fontweight='bold')

# Plot where the arrival time is
arrival_line = ax.axvline(x=arrival_time, c='red', label='Abs. Arrival')
ax.legend(handles=[arrival_line])

"""###Read Miniseed"""

mseed_file = f'{data_directory}{test_filename}.mseed'
st = read(mseed_file)
st

"""#### header info"""

st[0].stats

# This is how you get the data and the time, which is in seconds
tr = st.traces[0].copy()
tr_times = tr.times()
tr_data = tr.data

# Start time of trace (another way to get the relative arrival time using datetime)
starttime = tr.stats.starttime.datetime
arrival = (arrival_time - starttime).total_seconds()
arrival

"""### Machine Learning"""

train_data = pd.DataFrame(cat, columns=['filename', 'time_rel(sec)'])
print(train_data)

test_filename = train_data['filename']
print(test_filename)

"""####import train data"""

time_rel = cat['time_rel(sec)']

print(time_rel)

data_directory = '/content/drive/MyDrive/space_apps_2024_seismic_detection/data/lunar/training/data/S12_GradeA/'

x = []
time = [] #relative time in training datasets
velocity = [] #velocity in training datasets
for i in range (len(cat.filename)):
  mseed_file = f'{data_directory}{cat.filename[i]}.mseed'
  st = read(mseed_file)
  tr = st.traces[0].copy()
  tr_times = tr.times()
  tr_data = tr.data
  x.append([tr_times, tr_data])
  time.append(tr_times)
  velocity.append(tr_data)

# plotting the data to see if the files are imported correctly
plt.plot(time[6], velocity[6])
plt.xlabel('Time (s)')
plt.ylabel('Velocity (m/s)')
plt.title(f'{cat.filename[6]}')

"""#### Import test data"""

test_data_directory = '/content/drive/MyDrive/space_apps_2024_seismic_detection/data/lunar/test/data/'

dir = ['S12_GradeB', 'S15_GradeA', 'S15_GradeB', 'S16_GradeA', 'S16_GradeB'] #list of test data folders
dir_list = []
for i in range(len(dir)):
  dir_list.append(test_data_directory+dir[i])
print(dir_list)

file_list = []
for folder in dir_list:
  files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.mseed')]
  file_list.extend(files)

len(file_list)

test_time = []
test_velocity = []
test_x = []
for i in file_list:
  st = read(i)
  tr = st.traces[0].copy()
  tr_times = tr.times()
  tr_data = tr.data
  test_x.append([tr_times, tr_data])
  test_time.append(tr_times)
  test_velocity.append(tr_data)
#print(test_x)

"""#### Preparing the data

##### Padding the data
"""

max_len = 0
for i in range(len(velocity)):
  #print(len(velocity[i]))
  if len(velocity[i]) > max_len:
    # print(i)
    max_len = len(velocity[i])

for i in range(len(test_velocity)):
  #print(len(velocity[i]))
  if len(test_velocity[i]) > max_len:
    # print(i)
    max_len = len(test_velocity[i])

print('maximum number of data points in a file=',max_len)

#to have test and training data in the same dimensions
for i in range(len(velocity)):
  velocity[i] = np.pad(velocity[i], (0, max_len - len(velocity[i])), mode='constant')
  time[i] = np.pad(time[i], (0, max_len - len(time[i])), mode='constant')

for i in range(len(test_velocity)):
  test_velocity[i] = np.pad(test_velocity[i], (0, max_len - len(test_velocity[i])), mode='constant')
  test_time[i] = np.pad(test_time[i], (0, max_len - len(test_time[i])), mode='constant')

vel = np.array(velocity)
print('shape of traning velocities = ', vel.shape)

test_vel = np.array(test_velocity)
print('shape of testing velocities = ', test_vel.shape)

"""##### Data Augmentation for training datasets
As there isn't enough data, data augmentation is needed to train the LSTM model.

Unfortunately, although these codes work, the kernel will crash due to RAM issues. If you would like to do data augmentation, I recommend trying running the magnitude shifting/time shifting no more than twice in each loop.
"""

# finding time steps between the relative timestamps
interval = time[0][1]-time[0][0]
print(interval)

#Magnitude Scaling Method
scale_factor = np.random.uniform(low=0.8, high=1.2)
#augmented_data = train_x * scale_factor

def magnitude_shift(data):
  for i in range(len(data)):
    scale_factor = np.random.uniform(0,1)
    data[i] = data[i] * scale_factor
  return data

def time_shift(data, shift):
  for i in range(len(data)):
    data[i] = np.roll(data[i], shift, axis=0)
  return data

#To repeat magnitude shifting:
def repeat_func_vel(times, data1): #data needs to be duplicated before passing into the function as the function update the original data
    for i in range(times): #number of times it creates new datasets = 'times+1'
      data2 = magnitude_shift(data1)
      data1 = np.concatenate([data1, data2], axis=0)
    return data1

def repeat_func_time(times, data1):
  for i in range(times):
    data2 = data1.copy()
    data1 = np.concatenate([data1, data1], axis=0)
  return data1

# To repeat magnitude shifting
num_mag = 1 #number of magnitude shifting done
augmented_vel = vel.copy()
augmented_time = time.copy()

augmented_vel = repeat_func_vel(num_mag, augmented_vel)
augmented_time = repeat_func_time(num_mag, augmented_time)

print('shape of augmented training velocity is',augmented_vel.shape)
print('shape of augmented training time is',augmented_time.shape)

#To repeat time shifting
for j in range(1): #number of time shifting done
  train_aug = time_shift(vel, shift=j)
  augmented_vel = np.concatenate([augmented_vel, train_aug], axis=0)
  time_aug = time + j*interval
  augmented_time = np.concatenate([augmented_time, time_aug], axis=0)


print('shape of augmented training velocity is',augmented_vel.shape)
print('shape of augmented training time is',augmented_time.shape)

"""##### Scaling the data"""

from sklearn import preprocessing
from sklearn.preprocessing import minmax_scale
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

scaler = preprocessing.MinMaxScaler()
#scaler = preprocessing.StandardScaler()

# Run this if you applied data augmentation, else, use the cell below
#scale the train data
augmented_vel_transposed = augmented_vel.T
# Apply MinMaxScaler to the transposed data
scaled_augmented_vel_transposed = scaler.fit_transform(augmented_vel_transposed)

# Transpose back to the original shape
scaled_augmented_vel = scaled_augmented_vel_transposed.T

print(scaled_augmented_vel.shape)  # Check for correct shape

#scale the train data
vel_transposed = vel.T

# Apply MinMaxScaler to the transposed data
scaled_vel_transposed = scaler.fit_transform(vel_transposed)

# Transpose back to the original shape
scaled_vel = scaled_vel_transposed.T

print(scaled_vel.shape)  # Check for correct shape

#scale the test data
test_vel_transposed = test_vel.T

# Apply MinMaxScaler to the transposed data
scaled_test_vel_transposed = scaler.fit_transform(test_vel_transposed)

# Transpose back to the original shape
scaled_test_vel = scaled_test_vel_transposed.T

print(scaled_test_vel.shape) # check for correct shape

# Checking the shape of scaled data with original data
print(scaled_vel[6])
plt.figure(figsize=(10, 6))
plt.subplot(1,2,1, title = 'Scaled Velocity')
plt.xlabel('Relative time(s)')
plt.ylabel('Scaled Velocity')
plt.plot(time[6],scaled_vel[6])

plt.subplot(1,2,2, title='Original Velocity')
plt.xlabel('Relative time(s)')
plt.ylabel('Original Velocity')
plt.plot(time[6],velocity[6])

"""#### SVR model"""

train_x = scaled_vel
train_y = time_rel
test_x = scaled_test_vel

print(train_x.shape)
print(train_y.shape)
print(test_x.shape)

from sklearn.svm import SVR
model = SVR(kernel='rbf', C=1.0, epsilon=0.1)

model.fit(train_x, train_y)

predict_time = model.predict(test_x)



"""#### LSTM Model"""

# Use this tab if you applied data augmentation, else use the tab below
# checking for the right shape of training and test data
train_x = scaled_augmented_vel
train_y = augmented_time
test_x = scaled_test_vel

train_x = np.expand_dims(train_x, axis=1)
train_y = np.expand_dims(train_y, axis=1)
test_x = np.expand_dims(test_x, axis=1)

print('shape of train_x is',train_x.shape)
print('shape of train_y is',train_y.shape)
print('shape of test_x is',test_x.shape)

train_x = scaled_vel
train_y = time
test_x = scaled_test_vel

train_x = np.expand_dims(train_x, axis=1)
train_y = np.expand_dims(train_y, axis=1)
test_x = np.expand_dims(test_x, axis=1)

print('shape of train_x is',train_x.shape)
print('shape of train_y is',train_y.shape)
print('shape of test_x is',test_x.shape)

BatchSize = 16
Nepochs = 150
learning_rate = 0.01
ValidationSplit = 0.3


model = tf.keras.models.Sequential([
tf.keras.layers.LSTM(64, activation='tanh', return_sequences=True),

tf.keras.layers.LSTM(32, activation='tanh', return_sequences=True),
tf.keras.layers.Dropout(0.2),

#tf.keras.layers.LSTM(16,  return_sequences=True),

tf.keras.layers.Flatten(),
#tf.keras.layers.Dense(32,activation='linear'),

tf.keras.layers.Dense(16,activation='linear'),
tf.keras.layers.Dropout(0.2),
tf.keras.layers.Dense(1)#, activation='linear')
])

loss_fn = tf.keras.losses.MSE

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss=loss_fn, metrics=['mae'])

# Use augmented data in your model training

history = model.fit(x=train_x, y=train_y, validation_split=ValidationSplit,
                    batch_size=BatchSize, epochs=Nepochs)

model.summary()
print(history.history.keys())

plt.figure(figsize=(6, 4))
plt.plot(history.history['loss'], label='training loss')
plt.plot(history.history['val_loss'], label = 'validation loss')
plt.legend(fontsize=14)
plt.title('Loss function', fontsize=16)
plt.xlabel('No. of Epochs', fontsize=16)
plt.show()

plt.figure(figsize=(6, 4))
plt.plot(history.history['mae'], label='training mean absolute error')
plt.plot(history.history['val_mae'], label = 'validation mean absolute error')
plt.legend(fontsize=14)
plt.title('Mean Absolute Error', fontsize=16)
plt.xlabel('No. of Epochs', fontsize=16)
plt.show()

"""### Using the model to predict the results of test datasets"""

predict_time = model.predict(test_x)
#print(predict_time)

#for LSTM model
test_prediction = []
for i in range(len(test_x)):
    predict = predict_time[i][0]
    test_prediction.append(predict)

#for SVR model

test_prediction = predict_time

"""#### Plotting the prediction of the models"""

i =7
fig,ax = plt.subplots(1,1,figsize=(10,3))

# Plot trace
ax.plot(test_time[i],test_velocity[i])

# Mark detection
ax.axvline(x = test_prediction[i], color='red',label='Predicted Rel. Arrival')
ax.legend(loc='upper left', fontsize=12)

# Make the plot pretty
ax.set_xlim([min(test_time[i]),max(test_time[i])])
ax.set_ylabel('Velocity (m/s)', fontsize=12)
ax.set_xlabel('Time (s)', fontsize=12)
#ax.set_title(f'{test_filename}', fontweight='bold')

"""#### Running the model on the train data and compare with the actual arrival time"""

#SVR model
train_prediction = model.predict(train_x)

#LSTM model
predict_train = model.predict(train_x)
train_prediction = []
train_prediction = []
for i in range(len(train_x)):
    predict = predict_train[i][0]
    train_prediction.append(predict)

i =8
fig,ax = plt.subplots(1,1,figsize=(10,3))

# Plot trace
ax.plot(time[i], velocity[i])

# Mark detection
ax.axvline(x = train_prediction[i], color='red',label='Trained Rel. Arrival')
ax.axvline(x = time_rel[i], color='blue',label='True Rel. Arrival')
ax.legend(loc='upper left', fontsize=14)

# Make the plot pretty
ax.set_xlim([min(time[i]),max(time[i])])
ax.set_ylabel('Velocity (m/s)', fontsize=12)
ax.set_xlabel('Time (s)', fontsize=12)
#ax.set_title(f'{test_filename}', fontweight='bold')

"""### Extra seismic events data"""

!pip install pyweed